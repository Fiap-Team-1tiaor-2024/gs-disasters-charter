{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dwx8WYvcc_uR"
   },
   "source": [
    "- Configurações iniciais, imports e definição da pasta de trabalho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i3Kn8kVedjbR"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    print(\"Google Drive montado com sucesso.\")\n",
    "except ModuleNotFoundError:\n",
    "    print(\"AVISO: Não está no ambiente Google Colab ou erro ao montar Drive.\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao montar o Google Drive: {e}\")\n",
    "\n",
    "DRIVE_BASE_PATH_S3 = \"/content/drive/MyDrive/Colab Notebooks/FIAP/Global\"\n",
    "\n",
    "if not os.path.isdir(DRIVE_BASE_PATH_S3):\n",
    "  print(f\"AVISO: Diretório base '{DRIVE_BASE_PATH_S3}' não encontrado no Drive. Verifique os caminhos.\")\n",
    "\n",
    "CAMINHO_ARQUIVO_CSV = os.path.join(DRIVE_BASE_PATH_S3, \"dados-inmet-sp.csv\")\n",
    "\n",
    "CAMINHO_DA_PASTA_DE_IMAGENS = os.path.join(DRIVE_BASE_PATH_S3, \"images\")\n",
    "\n",
    "LISTA_NOMES_IMAGENS = []\n",
    "EXTENSOES_IMAGEM_VALIDAS = ('.jpg', '.jpeg', '.png')\n",
    "\n",
    "print(f\"Tentando ler imagens da pasta: {CAMINHO_DA_PASTA_DE_IMAGENS}\")\n",
    "if os.path.exists(CAMINHO_DA_PASTA_DE_IMAGENS) and os.path.isdir(CAMINHO_DA_PASTA_DE_IMAGENS):\n",
    "    try:\n",
    "        for nome_arquivo_na_pasta in os.listdir(CAMINHO_DA_PASTA_DE_IMAGENS):\n",
    "            if nome_arquivo_na_pasta.lower().endswith(EXTENSOES_IMAGEM_VALIDAS):\n",
    "                LISTA_NOMES_IMAGENS.append(nome_arquivo_na_pasta)\n",
    "\n",
    "        if LISTA_NOMES_IMAGENS:\n",
    "            print(f\"{len(LISTA_NOMES_IMAGENS)} imagens encontradas na pasta.\")\n",
    "            print(\"Amostra de nomes de imagens carregados:\", LISTA_NOMES_IMAGENS[:5]) # Mostra os 5 primeiros\n",
    "        else:\n",
    "            print(\"Nenhuma imagem encontrada com as extensões válidas na pasta especificada.\")\n",
    "            print(\"Verifique o caminho da pasta e se as imagens têm extensões como .jpg, .jpeg, ou .png.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao listar arquivos da pasta '{CAMINHO_DA_PASTA_DE_IMAGENS}': {e}\")\n",
    "        LISTA_NOMES_IMAGENS = [] # Garante lista vazia em caso de erro na leitura\n",
    "else:\n",
    "    print(f\"Erro: A pasta de imagens '{CAMINHO_DA_PASTA_DE_IMAGENS}' não foi encontrada ou não é um diretório válido. Verifique o caminho.\")\n",
    "    LISTA_NOMES_IMAGENS = [] # Garante lista vazia\n",
    "\n",
    "# Definindo os limiares de alerta\n",
    "MEUS_LIMIARES_DE_ALERTA = [\n",
    "    {'coluna_acumulado': 'prec_acum_1h', 'valor_mm': 25, 'nivel_alerta': 'ALERTA MÁXIMO', 'mensagem': 'Chuva MUITO FORTE em 1h (>25mm)'},\n",
    "    {'coluna_acumulado': 'prec_acum_1h', 'valor_mm': 15, 'nivel_alerta': 'ALERTA', 'mensagem': 'Chuva FORTE em 1h (>15mm)'},\n",
    "    {'coluna_acumulado': 'prec_acum_24h', 'valor_mm': 100, 'nivel_alerta': 'ALERTA MÁXIMO', 'mensagem': 'Acumulado >100mm em 24h - RISCO ALTO'},\n",
    "    {'coluna_acumulado': 'prec_acum_24h', 'valor_mm': 70, 'nivel_alerta': 'ALERTA', 'mensagem': 'Acumulado >70mm em 24h - RISCO MODERADO'},\n",
    "    {'coluna_acumulado': 'prec_acum_24h', 'valor_mm': 40, 'nivel_alerta': 'ATENÇÃO', 'mensagem': 'Acumulado >40mm em 24h - Atenção'},\n",
    "    {'coluna_acumulado': 'prec_acum_72h', 'valor_mm': 150, 'nivel_alerta': 'ALERTA', 'mensagem': 'Acumulado >150mm em 72h (solo saturado)'},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a0pcax4_BZbe"
   },
   "outputs": [],
   "source": [
    "def carregar_dados_climaticos(\n",
    "    caminho_arquivo_csv,\n",
    "    nome_coluna_data_completa='data',\n",
    "    nome_coluna_hora_completa='hora',\n",
    "    nome_coluna_precipitacao='precipitacao_total',\n",
    "    nome_coluna_estacao='estacao',\n",
    "    nome_coluna_latitude='latitude',\n",
    "    nome_coluna_longitude='longitude',\n",
    "    nome_coluna_municipio='municipio',\n",
    "    nome_coluna_estado='estado',\n",
    "    linhas_cabecalho=0,\n",
    "    separador=',',\n",
    "    codificacao='utf-8',\n",
    "    formato_decimal='.'\n",
    "):\n",
    "    \"\"\"\n",
    "    Carrega e pré-processa dados climáticos de um arquivo CSV.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(\n",
    "            caminho_arquivo_csv,\n",
    "            skiprows=linhas_cabecalho,\n",
    "            sep=separador,\n",
    "            encoding=codificacao,\n",
    "            decimal=formato_decimal,\n",
    "            na_values=['', ' ', 'NULL', '-9999']\n",
    "        )\n",
    "        print(f\"Arquivo '{caminho_arquivo_csv}' lido com sucesso.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Erro: O arquivo '{caminho_arquivo_csv}' não foi encontrado.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao ler o arquivo CSV: {e}\")\n",
    "        return None\n",
    "\n",
    "    colunas_map = {\n",
    "        'data_str': nome_coluna_data_completa,\n",
    "        'hora_str': nome_coluna_hora_completa,\n",
    "        'precipitacao': nome_coluna_precipitacao,\n",
    "        'estacao': nome_coluna_estacao,\n",
    "        'latitude': nome_coluna_latitude,\n",
    "        'longitude': nome_coluna_longitude,\n",
    "        'municipio': nome_coluna_municipio,\n",
    "        'estado': nome_coluna_estado\n",
    "    }\n",
    "\n",
    "    colunas_para_selecionar_renomear = {}\n",
    "    colunas_obrigatorias = ['data_str', 'hora_str', 'precipitacao', 'estacao']\n",
    "\n",
    "    for nome_logico, nome_no_arquivo in colunas_map.items():\n",
    "        if nome_no_arquivo in df.columns:\n",
    "            colunas_para_selecionar_renomear[nome_no_arquivo] = nome_logico\n",
    "        elif nome_logico in colunas_obrigatorias:\n",
    "            print(f\"Erro: A coluna obrigatória '{nome_no_arquivo}' (para {nome_logico}) não foi encontrada.\")\n",
    "            print(f\"Colunas disponíveis: {df.columns.tolist()}\")\n",
    "            return None\n",
    "\n",
    "    try:\n",
    "        colunas_existentes_no_csv_para_usar = [col_csv for col_csv in colunas_para_selecionar_renomear.keys() if col_csv in df.columns]\n",
    "        df_selecionado = df[colunas_existentes_no_csv_para_usar].copy()\n",
    "        df_selecionado.rename(columns=colunas_para_selecionar_renomear, inplace=True)\n",
    "    except KeyError as e:\n",
    "        print(f\"Erro ao selecionar/renomear colunas. Detalhe: {e}\")\n",
    "        return None\n",
    "\n",
    "    if 'precipitacao' in df_selecionado.columns:\n",
    "        df_selecionado['precipitacao'] = pd.to_numeric(df_selecionado['precipitacao'], errors='coerce')\n",
    "        df_selecionado['precipitacao'] = df_selecionado['precipitacao'].fillna(0) # Corrigido\n",
    "    else:\n",
    "        print(\"Erro: Coluna 'precipitacao' não está no DataFrame selecionado após o mapeamento.\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        datetime_str = df_selecionado['data_str'].astype(str) + ' ' + df_selecionado['hora_str'].astype(str)\n",
    "        df_selecionado['timestamp'] = pd.to_datetime(datetime_str, errors='coerce')\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao converter data e hora para timestamp: {e}\")\n",
    "        return None\n",
    "\n",
    "    df_selecionado.dropna(subset=['timestamp'], inplace=True)\n",
    "    df_selecionado.set_index('timestamp', inplace=True)\n",
    "\n",
    "    colunas_finais_desejadas = ['precipitacao', 'estacao', 'latitude', 'longitude', 'municipio', 'estado']\n",
    "    colunas_finais_presentes = [col for col in colunas_finais_desejadas if col in df_selecionado.columns]\n",
    "    df_final = df_selecionado[colunas_finais_presentes]\n",
    "\n",
    "    print(\"\\n--- Amostra dos dados processados ---\")\n",
    "    print(df_final.head())\n",
    "    print(\"\\n--- Informações dos dados processados ---\")\n",
    "    df_final.info()\n",
    "\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xiP3IOdCBaW4"
   },
   "outputs": [],
   "source": [
    "def calcular_precipitacao_acumulada(df, janelas_horas=[1, 3, 6, 12, 24, 72]):\n",
    "    \"\"\"\n",
    "    Calcula a precipitação acumulada, agrupando por 'estacao'.\n",
    "    \"\"\"\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        print(\"Erro: A entrada não é um DataFrame do Pandas.\")\n",
    "        return None\n",
    "    if 'precipitacao' not in df.columns:\n",
    "        print(\"Erro: Coluna 'precipitacao' não encontrada.\")\n",
    "        return None\n",
    "    if 'estacao' not in df.columns:\n",
    "        print(\"Erro: Coluna 'estacao' não encontrada. Cálculo por estação não é possível.\")\n",
    "        return None\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        print(\"Erro: O DataFrame não possui um DatetimeIndex.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"\\nCalculando precipitação acumulada para janelas: {janelas_horas} horas...\")\n",
    "    df_copia = df.copy()\n",
    "    df_copia.sort_values(by=['estacao', df_copia.index.name], inplace=True)\n",
    "\n",
    "    for janela_h in janelas_horas:\n",
    "        nome_coluna_acumulada = f'prec_acum_{janela_h}h'\n",
    "        print(f\"  Calculando para {janela_h}h...\")\n",
    "        df_copia[nome_coluna_acumulada] = df_copia.groupby('estacao')['precipitacao'] \\\n",
    "                                               .rolling(window=janela_h, min_periods=1) \\\n",
    "                                               .sum() \\\n",
    "                                               .reset_index(level='estacao', drop=True)\n",
    "    print(\"\\nCálculo de precipitação acumulada concluído.\")\n",
    "    return df_copia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpDoBpecBf46"
   },
   "outputs": [],
   "source": [
    "def gerar_alertas_precipitacao(df_dados, limiares):\n",
    "    \"\"\"\n",
    "    Gera alertas de precipitação com base em limiares definidos, usando 'estacao'.\n",
    "    \"\"\"\n",
    "    if not isinstance(df_dados, pd.DataFrame) or not isinstance(df_dados.index, pd.DatetimeIndex):\n",
    "        print(\"Erro: df_dados deve ser um DataFrame com DatetimeIndex.\")\n",
    "        return pd.DataFrame()\n",
    "    if not isinstance(limiares, list) or not all(isinstance(item, dict) for item in limiares):\n",
    "        print(\"Erro: limiares deve ser uma lista de dicionários.\")\n",
    "        return pd.DataFrame()\n",
    "    if 'estacao' not in df_dados.columns:\n",
    "        print(\"Erro: Coluna 'estacao' não encontrada no DataFrame.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    alertas_gerados = []\n",
    "    print(\"\\nVerificando alertas...\")\n",
    "    colunas_extras_possiveis = ['latitude', 'longitude', 'municipio', 'estado']\n",
    "\n",
    "    for _, linha in df_dados.iterrows():\n",
    "        timestamp = linha.name\n",
    "        nome_estacao = linha['estacao']\n",
    "        dados_extras_estacao = {col_extra: linha.get(col_extra) for col_extra in colunas_extras_possiveis if col_extra in linha}\n",
    "\n",
    "        for limiar in limiares:\n",
    "            coluna_gatilho = limiar['coluna_acumulado']\n",
    "            valor_limite = limiar['valor_mm']\n",
    "            nivel = limiar['nivel_alerta']\n",
    "            msg_template = limiar['mensagem']\n",
    "\n",
    "            if coluna_gatilho in linha:\n",
    "                valor_observado = linha[coluna_gatilho]\n",
    "                if pd.notna(valor_observado) and valor_observado > valor_limite:\n",
    "                    alerta = {\n",
    "                        'timestamp': timestamp,\n",
    "                        'estacao': nome_estacao,\n",
    "                        'nivel_alerta': nivel,\n",
    "                        'descricao_alerta': msg_template,\n",
    "                        'coluna_trigger': coluna_gatilho,\n",
    "                        'valor_observado_mm': valor_observado,\n",
    "                        'limiar_mm': valor_limite\n",
    "                    }\n",
    "                    alerta.update(dados_extras_estacao)\n",
    "                    alertas_gerados.append(alerta)\n",
    "\n",
    "    if not alertas_gerados:\n",
    "        print(\"Nenhum alerta gerado com os critérios atuais.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df_alertas = pd.DataFrame(alertas_gerados)\n",
    "    niveis_ordem = ['ATENÇÃO', 'ALERTA', 'ALERTA MÁXIMO']\n",
    "    if not df_alertas.empty and 'nivel_alerta' in df_alertas.columns:\n",
    "        df_alertas['nivel_alerta'] = pd.Categorical(df_alertas['nivel_alerta'], categories=niveis_ordem, ordered=True)\n",
    "        df_alertas.sort_values(by=['timestamp', 'estacao', 'nivel_alerta'], ascending=[True, True, False], inplace=True)\n",
    "\n",
    "    print(f\"{len(df_alertas)} alertas gerados.\")\n",
    "    return df_alertas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pVBVZs_WBj0O"
   },
   "outputs": [],
   "source": [
    "def extrair_data_de_nome_arquivo(nome_arquivo):\n",
    "    \"\"\"\n",
    "    Tenta extrair uma data (AAAA-MM-DD ou DD-MM-AAAA) de um nome de arquivo.\n",
    "    \"\"\"\n",
    "    match_iso = re.search(r'(\\d{4})-(\\d{2})-(\\d{2})', nome_arquivo)\n",
    "    if match_iso:\n",
    "        ano, mes, dia = map(int, match_iso.groups())\n",
    "        try: return datetime(ano, mes, dia).date()\n",
    "        except ValueError: pass\n",
    "    match_dmY = re.search(r'(\\d{2})-(\\d{2})-(\\d{4})', nome_arquivo)\n",
    "    if match_dmY:\n",
    "        dia, mes, ano = map(int, match_dmY.groups())\n",
    "        try: return datetime(ano, mes, dia).date()\n",
    "        except ValueError: pass\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p-MlH2iEBlTZ"
   },
   "outputs": [],
   "source": [
    "def extrair_data_de_nome_arquivo(nome_arquivo):\n",
    "    \"\"\"\n",
    "    Tenta extrair uma data (AAAA-MM-DD ou DD-MM-AAAA) de um nome de arquivo.\n",
    "    \"\"\"\n",
    "    match_iso = re.search(r'(\\d{4})-(\\d{2})-(\\d{2})', nome_arquivo)\n",
    "    if match_iso:\n",
    "        ano, mes, dia = map(int, match_iso.groups())\n",
    "        try: return datetime(ano, mes, dia).date()\n",
    "        except ValueError: pass\n",
    "    match_dmY = re.search(r'(\\d{2})-(\\d{2})-(\\d{4})', nome_arquivo)\n",
    "    if match_dmY:\n",
    "        dia, mes, ano = map(int, match_dmY.groups())\n",
    "        try: return datetime(ano, mes, dia).date()\n",
    "        except ValueError: pass\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-9hjlBf0CrlU"
   },
   "outputs": [],
   "source": [
    "def correlacionar_alertas_com_imagens(df_alertas, lista_nomes_imagens):\n",
    "    \"\"\"\n",
    "    Correlaciona alertas de precipitação com imagens baseado na data, esperando 'estacao'.\n",
    "    \"\"\"\n",
    "    if not isinstance(df_alertas, pd.DataFrame) or 'timestamp' not in df_alertas.columns:\n",
    "        print(\"Erro: df_alertas deve ser um DataFrame com a coluna 'timestamp'.\")\n",
    "        return pd.DataFrame()\n",
    "    if 'estacao' not in df_alertas.columns:\n",
    "         print(\"Erro: df_alertas deve conter a coluna 'estacao'.\")\n",
    "         return pd.DataFrame()\n",
    "    if not isinstance(lista_nomes_imagens, list):\n",
    "        print(\"Erro: lista_nomes_imagens deve ser uma lista.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    mapa_data_para_imagens = {}\n",
    "    print(\"\\nProcessando nomes de arquivos de imagem...\")\n",
    "    for nome_imagem in lista_nomes_imagens:\n",
    "        data_imagem = extrair_data_de_nome_arquivo(nome_imagem)\n",
    "        if data_imagem:\n",
    "            print(f\"  Imagem: '{nome_imagem}' -> Data: {data_imagem}\")\n",
    "            if data_imagem not in mapa_data_para_imagens:\n",
    "                mapa_data_para_imagens[data_imagem] = []\n",
    "            mapa_data_para_imagens[data_imagem].append(nome_imagem)\n",
    "        else:\n",
    "            print(f\"  Imagem: '{nome_imagem}' -> Data não extraída.\")\n",
    "\n",
    "    if not mapa_data_para_imagens:\n",
    "        print(\"Nenhuma data pôde ser extraída dos nomes das imagens.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(\"\\nCorrelacionando alertas com imagens...\")\n",
    "    alertas_correlacionados = []\n",
    "    df_alertas_copia = df_alertas.copy()\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df_alertas_copia['timestamp']):\n",
    "         df_alertas_copia['timestamp'] = pd.to_datetime(df_alertas_copia['timestamp'], errors='coerce')\n",
    "    df_alertas_copia['data_alerta_obj'] = df_alertas_copia['timestamp'].dt.date\n",
    "\n",
    "    for _, alerta_linha in df_alertas_copia.iterrows():\n",
    "        data_alerta = alerta_linha['data_alerta_obj']\n",
    "        if pd.isna(data_alerta): continue\n",
    "\n",
    "        if data_alerta in mapa_data_para_imagens:\n",
    "            imagens_do_dia = mapa_data_para_imagens[data_alerta]\n",
    "            for nome_img_associada in imagens_do_dia:\n",
    "                alerta_com_imagem = alerta_linha.to_dict()\n",
    "                alerta_com_imagem['imagem_associada'] = nome_img_associada\n",
    "                alertas_correlacionados.append(alerta_com_imagem)\n",
    "\n",
    "    if not alertas_correlacionados:\n",
    "        print(\"Nenhuma correlação encontrada entre alertas e imagens.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df_resultado_correlacao = pd.DataFrame(alertas_correlacionados)\n",
    "    if 'data_alerta_obj' in df_resultado_correlacao.columns:\n",
    "        df_resultado_correlacao.drop(columns=['data_alerta_obj'], inplace=True)\n",
    "\n",
    "    print(f\"{len(df_resultado_correlacao)} alertas foram correlacionados com imagens.\")\n",
    "    return df_resultado_correlacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lMG5opg0Bn5J"
   },
   "outputs": [],
   "source": [
    "# 1. Carregar os dados\n",
    "dados_base = carregar_dados_climaticos(CAMINHO_ARQUIVO_CSV)\n",
    "\n",
    "# 2. Calcular precipitação acumulada\n",
    "if dados_base is not None:\n",
    "    dados_com_acumulados = calcular_precipitacao_acumulada(dados_base)\n",
    "else:\n",
    "    dados_com_acumulados = None\n",
    "\n",
    "# 3. Gerar alertas\n",
    "if dados_com_acumulados is not None:\n",
    "    alertas_gerados = gerar_alertas_precipitacao(dados_com_acumulados, MEUS_LIMIARES_DE_ALERTA)\n",
    "else:\n",
    "    alertas_gerados = None\n",
    "\n",
    "# 4. Correlacionar alertas com imagens\n",
    "if alertas_gerados is not None and not alertas_gerados.empty:\n",
    "    alertas_finais_com_imagens = correlacionar_alertas_com_imagens(alertas_gerados, LISTA_NOMES_IMAGENS)\n",
    "\n",
    "    if alertas_finais_com_imagens is not None and not alertas_finais_com_imagens.empty:\n",
    "        print(\"\\n--- RESULTADO FINAL: Amostra de Alertas Correlacionados com Imagens ---\")\n",
    "        pd.set_option('display.max_columns', None)\n",
    "        print(alertas_finais_com_imagens.head())\n",
    "        pd.reset_option('display.max_columns')\n",
    "\n",
    "        print(f\"\\nTotal de alertas finais correlacionados com imagens: {len(alertas_finais_com_imagens)}\")\n",
    "\n",
    "        # Análise mais detalhada dos resultados correlacionados\n",
    "        print(\"\\n--- Análise Detalhada dos Alertas Correlacionados ---\")\n",
    "        for nome_imagem, grupo in alertas_finais_com_imagens.groupby('imagem_associada'):\n",
    "            data_img_obj = extrair_data_de_nome_arquivo(nome_imagem)\n",
    "            print(f\"\\nImagem: {nome_imagem} (Data: {data_img_obj})\")\n",
    "            print(f\"  {len(grupo)} alertas associados neste dia:\")\n",
    "            print(grupo[['timestamp', 'estacao', 'municipio', 'nivel_alerta', 'descricao_alerta', 'valor_observado_mm']].head(10)) # Mostrar até 10 por imagem\n",
    "            print(\"-\" * 40)\n",
    "    elif alertas_gerados is not None: # Se houve alertas mas nenhuma correlação\n",
    "        print(\"\\nNenhuma correlação encontrada entre os alertas gerados e as imagens fornecidas.\")\n",
    "    # else: # Se não houve alertas, a mensagem já foi dada por gerar_alertas_precipitacao\n",
    "\n",
    "elif dados_com_acumulados is not None: # Se não houve alertas\n",
    "     print(\"Nenhum alerta foi gerado a partir dos dados de chuva acumulada.\")\n",
    "\n",
    "else: # Se o carregamento inicial falhou\n",
    "    print(\"Processamento interrompido devido à falha no carregamento dos dados iniciais.\")\n",
    "\n",
    "print(\"\\n--- Fim do Processamento ---\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNMssIspwYDHYsGP9T3gJfH",
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
